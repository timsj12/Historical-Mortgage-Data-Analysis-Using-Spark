{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6062efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, countDistinct, when, isnan, count, first\n",
    "from pyspark.sql.types import IntegerType, StringType, DoubleType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Imputer, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['HADOOP_HOME'] = r'C:\\Users\\timj3\\hadoop'\n",
    "os.environ['hadoop.home.dir'] = r'C:\\Users\\timj3\\hadoop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03aa76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define functions\n",
    "\n",
    "def initialize_spark():\n",
    "    \"\"\"\n",
    "    Initialize and return a Spark session.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MortageGroupProject\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "        .config(\"spark.eventLog.dir\", r\"\\Users\\timj3\\SparkLogs\")  \\\n",
    "        .config(\"spark.eventLog.gcMetrics.youngGenerationGarbageCollectors\", \"G1 Young Generation\") \\\n",
    "        .config(\"spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\", \"G1 Old Generation\") \\\n",
    "        .config(\"spark.sql.codegen.wholeStage\", False) \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "#         %%configure -f\n",
    "#         {\n",
    "#         \"driverMemory\": \"1000M\",\n",
    "#         \"executorMemory\": \"2000M\",\n",
    "#         \"executorCores\": 1,\n",
    "#         \"numExecutors\": 10\n",
    "#         }\n",
    "    \n",
    "    #spark = SparkSession.builder \\\n",
    "#    .appName(\"MortageGroupProject\") \\\n",
    "#    .master(\"local[*]\") \\\n",
    "#    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "#    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "#    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "#    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "#    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "#    .config(\"spark.memory.offHeap.size\", \"1g\") \\\n",
    "#    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "#    .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "#    .config(\"spark.eventLog.dir\", \"/Users/mattcurtis/Documents/CS 5644/SparkLogs\") \\\n",
    "#    .config(\"spark.sql.codegen.wholeStage\", False) \\\n",
    "#    .getOrCreate()\n",
    "\n",
    "    # Hide all the warnings\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    return spark\n",
    "\n",
    "    #spark = SparkSession.builder \\\n",
    "#    .appName(\"MortageGroupProject\") \\\n",
    "#    .master(\"local[*]\") \\\n",
    "#    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "#    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "#    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "#    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "#    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "#    .config(\"spark.memory.offHeap.size\", \"1g\") \\\n",
    "#    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "#    .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "#    .config(\"spark.eventLog.dir\", \"/Users/mattcurtis/Documents/CS 5644/SparkLogs\") \\\n",
    "#    .config(\"spark.sql.codegen.wholeStage\", False) \\\n",
    "#    .getOrCreate()\n",
    "\n",
    "    # Hide all the warnings\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    return spark\n",
    "\n",
    "def print_execution_time(start_time, task_name):\n",
    "    \"\"\"\n",
    "    Prints the execution time for a given task in HH:MM:SS format.\n",
    "    \n",
    "    :param start_time: The start time in seconds (usually obtained using time.time()).\n",
    "    :param task_name: A string representing the name of the task.\n",
    "    \"\"\"\n",
    "    end_time = time.time()  # Record the current time as the end time\n",
    "    execution_time_seconds = end_time - start_time  # Calculate the execution time in seconds\n",
    "\n",
    "    # Convert seconds to HH:MM:SS format\n",
    "    minutes, seconds = divmod(execution_time_seconds, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    print(f\"{task_name} took: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
    "    \n",
    "def print_column_names_and_types(df: DataFrame):\n",
    "    \"\"\"\n",
    "    Print the names and data types of all columns in a Spark DataFrame.\n",
    "\n",
    "    :param df: The DataFrame whose column names and data types are to be printed.\n",
    "    \"\"\"\n",
    "    for column, datatype in df.dtypes:\n",
    "        print(f\"Column: {column}, Type: {datatype}\")\n",
    "        \n",
    "def print_row_count(df: DataFrame):\n",
    "    \"\"\"\n",
    "    Print the total number of rows in a Spark DataFrame.\n",
    "\n",
    "    :param df: The DataFrame for which to count and print the number of rows.\n",
    "    \"\"\"\n",
    "    row_count = df.count()\n",
    "    print(f\"Total number of rows in the DataFrame: {row_count:,}\")\n",
    "    \n",
    "def show_unique_values(data: DataFrame, column_name: str):\n",
    "    \"\"\"\n",
    "    Displays unique values of a specified column in a Spark DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The Spark DataFrame to analyze.\n",
    "    column_name (str): The name of the column for which to show unique values.\n",
    "    \"\"\"\n",
    "    unique_values = data.select(column_name).distinct()\n",
    "    unique_values.show()\n",
    "    \n",
    "def print_dataframe_info(data: DataFrame):\n",
    "    \"\"\"\n",
    "    prints list of columns in dataframe\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The Spark DataFrame to analyze.\n",
    "    \"\"\"\n",
    "    # Printing the columns\n",
    "    print(\"Columns left:\")\n",
    "    print(sorted(data.columns))\n",
    "    print(\"Number of Columns\", len(data.columns))\n",
    "    \n",
    "def drop_columns(df: DataFrame, columns_to_drop: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Drop specified columns from a PySpark DataFrame.\n",
    "\n",
    "    :param df: The DataFrame from which to drop columns.\n",
    "    :param columns_to_drop: A list of column names to drop.\n",
    "    :return: A DataFrame with the specified columns removed.\n",
    "    \"\"\"\n",
    "    for col in columns_to_drop:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(col)\n",
    "        else:\n",
    "            print(f\"Column '{col}' not found in DataFrame. drop_columns\")\n",
    "    return df\n",
    "\n",
    "def remove_na_rows(df: DataFrame, columns_to_drop: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Drops rows with the nulls values.\n",
    "\n",
    "    :param df: The DataFrame from which to drop columns.\n",
    "    :param columns_to_drop: A list of column names to drop.\n",
    "    :return: A DataFrame with the specified columns removed.\n",
    "    \"\"\"\n",
    "    data_columns = data.columns\n",
    "    for value in columns_to_drop[:]:\n",
    "        if value not in data_columns:\n",
    "            print(f\"Column {value} not found in DataFrame. remove_na_rows\")\n",
    "            columns_to_drop.remove(value)\n",
    "    return df.dropna(subset=columns_to_drop)\n",
    "\n",
    "def remove_exempt(df: DataFrame, columns_to_drop: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Drops rows that are 'Exempt'.\n",
    "\n",
    "    :param df: The DataFrame from which to drop columns.\n",
    "    :param columns_to_drop: A list of column names to drop.\n",
    "    :return: A DataFrame with the specified columns removed.\n",
    "    \"\"\"\n",
    "    for column in columns_to_drop:\n",
    "        if column in df.columns:\n",
    "            df = df.filter(df[column] != \"Exempt\")\n",
    "        else:\n",
    "            print(f\"Column '{column}' not found in DataFrame. remove_exempt\")\n",
    "    return df\n",
    "\n",
    "def balance_classes(df: DataFrame, column: str, target_count: int) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Balance the number of rows for two classes in a PySpark DataFrame.\n",
    "\n",
    "    :param df: The DataFrame to balance.\n",
    "    :param column: The column name for the class labels.\n",
    "    :param target_count: The target number of rows for each class.\n",
    "    :return: A balanced DataFrame.\n",
    "    \"\"\"\n",
    "    # Separate the DataFrame into two based on the class\n",
    "    df_class_0 = df.filter(F.col(column) == 0)\n",
    "    df_class_1 = df.filter(F.col(column) == 1)\n",
    "    \n",
    "    # Calculate the fraction to sample from the larger class\n",
    "    fraction = target_count / df_class_1.count()\n",
    "    fraction = min(1.0, fraction)  # Ensure the fraction is not more than 1\n",
    "\n",
    "    # Sample from the larger class to match the count of the smaller class\n",
    "    df_class_1_sampled = df_class_1.sample(withReplacement=False, fraction=fraction)\n",
    "\n",
    "    # Combine the DataFrames\n",
    "    return df_class_0.unionAll(df_class_1_sampled)\n",
    "\n",
    "def count_unique_values(df: DataFrame, column_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Count the occurrences of each unique value in a specified column of a PySpark DataFrame.\n",
    "\n",
    "    :param df: The DataFrame to analyze.\n",
    "    :param column_name: The name of the column to count unique values for.\n",
    "    :return: A DataFrame with counts of each unique value in the specified column.\n",
    "    \"\"\"\n",
    "    # Group by the specified column and count occurrences\n",
    "    value_counts = df.groupBy(column_name).count()\n",
    "\n",
    "    # Sort by the specified column for better readability\n",
    "    value_counts = value_counts.orderBy(column_name)\n",
    "\n",
    "    return value_counts\n",
    "\n",
    "def impute_and_replace_columns(df: DataFrame, columns_to_impute: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Impute missing values in specified columns of a DataFrame and replace the original columns with the imputed ones.\n",
    "\n",
    "    :param df: The DataFrame to impute.\n",
    "    :param columns_to_impute: List of column names to impute.\n",
    "    :return: A DataFrame with imputed values, where specified columns are replaced by their imputed versions.\n",
    "    \"\"\"\n",
    "    data_columns = data.columns\n",
    "    for value in columns_to_impute[:]:\n",
    "        if value not in data_columns:\n",
    "            print(f\"Column not in DataFrame: {value}. Imputer\")\n",
    "            one_hot_encoded_columns.remove(value)\n",
    "    \n",
    "    \n",
    "    # Create the imputer for specified columns\n",
    "    imputer = Imputer(\n",
    "        inputCols=columns_to_impute, \n",
    "        outputCols=[c + \"_imputed\" for c in columns_to_impute]\n",
    "    )    \n",
    "    # Fit the imputer model and transform the DataFrame\n",
    "    imputed_df = imputer.fit(df).transform(df)\n",
    "\n",
    "    # Loop through all columns in the imputed DataFrame\n",
    "    for col in imputed_df.columns:\n",
    "        # Check if the column name ends with \"_imputed\"\n",
    "        if col.endswith(\"_imputed\"):\n",
    "            # Extract the original column name\n",
    "            original_col_name = col.replace(\"_imputed\", \"\")\n",
    "\n",
    "            # Drop the original column from the DataFrame and rename the imputed column\n",
    "            imputed_df = imputed_df.drop(original_col_name).withColumnRenamed(col, original_col_name)\n",
    "\n",
    "    return imputed_df\n",
    "\n",
    "def convert_columns_to_numeric(data: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Convert columns in a DataFrame to numeric types where possible. Non-convertible columns are logged.\n",
    "\n",
    "    :param data: The Spark DataFrame to analyze and convert.\n",
    "    :return: The modified DataFrame with columns converted to numeric types where possible.\n",
    "    \"\"\"\n",
    "    non_numerical = []\n",
    "    for column in data.columns:\n",
    "        original_data = data.select(column)\n",
    "        try:\n",
    "            # First try to cast the column to IntegerType\n",
    "            data = data.withColumn(column, col(column).cast(IntegerType()))\n",
    "        except:\n",
    "            try:\n",
    "                # If integer conversion fails, try to cast to FloatType\n",
    "                data = data.withColumn(column, col(column).cast(FloatType()))\n",
    "            except:\n",
    "                # If both conversions fail, add the column to the non_numerical list\n",
    "                non_numerical.append(column)\n",
    "\n",
    "    # Log non-convertible columns\n",
    "    if non_numerical:\n",
    "        print(\"Could not convert the following columns to integer or float:\", non_numerical)\n",
    "\n",
    "    return data\n",
    "\n",
    "def drop_all_null_columns(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Check for columns with all null values in the given DataFrame and drop them.\n",
    "\n",
    "    :param df: The DataFrame to check for null values.\n",
    "    :return: A DataFrame with columns containing all nulls dropped.\n",
    "    \"\"\"\n",
    "    # Initialize a list for droppable columns\n",
    "    droppable_columns = []\n",
    "\n",
    "    for column in df.columns:\n",
    "        # Check if there is at least one non-null value in the column\n",
    "        first_non_null = df.filter(col(column).isNotNull()).select(column).first()\n",
    "        if first_non_null is not None:\n",
    "            # Skip to the next iteration as this column has at least one non-null value\n",
    "            print(f\"Skipping: {column}\")\n",
    "            continue\n",
    "        # If the first non-null value is not found, add the column to the droppable list\n",
    "        print(f\"All null values: {column}\")\n",
    "        droppable_columns.append(column)\n",
    "        \n",
    "    print(\"Final list of columns containing all nulls\")\n",
    "    print(droppable_columns)\n",
    "\n",
    "    # Drop columns that are entirely null\n",
    "    for column in droppable_columns:\n",
    "        df = df.drop(column)\n",
    "\n",
    "    return df\n",
    "\n",
    "def transform_and_filter_action_taken(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform the 'action_taken' column based on specific conditions, filter out certain rows,\n",
    "    and rename the transformed column.\n",
    "\n",
    "    :param df: The DataFrame containing the 'action_taken' column.\n",
    "    :return: A DataFrame with the transformed 'action_taken' column and filtered rows.\n",
    "    \"\"\"\n",
    "    # Transform the 'action_taken' column\n",
    "    transformed_df = df.withColumn(\"action_taken_transformed\",\n",
    "                                   when(col(\"action_taken\").isin([1, 2, 6, 8]), 1)\n",
    "                                   .when(col(\"action_taken\").isin([3, 7]), 0))\n",
    "    \n",
    "    # Drop rows with 'action_taken' as 4 or 5\n",
    "    filtered_df = transformed_df.filter(~col(\"action_taken\").isin([4, 5]))\n",
    "\n",
    "    # Drop the original 'action_taken' column and rename the transformed column\n",
    "    final_df = filtered_df.drop(\"action_taken\").withColumnRenamed(\"action_taken_transformed\", \"action_taken\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def find_columns_for_one_hot_encoding(data: DataFrame, potential_columns: list) -> list:\n",
    "    \"\"\"\n",
    "    Identify columns in a DataFrame that have more than one non-null value and are suitable for one-hot encoding.\n",
    "\n",
    "    :param data: The Spark DataFrame to analyze.\n",
    "    :param potential_columns: A list of column names to check for one-hot encoding suitability.\n",
    "    :return: A list of column names that have more than one non-null value and are suitable for one-hot encoding.\n",
    "    \"\"\"\n",
    "    one_hot_encodable_columns = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate through the list and check the count of distinct non-null values\n",
    "    for column in potential_columns:\n",
    "        distinct_count = data.agg(countDistinct(col(column)).alias(\"distinct_count\")).collect()[0][\"distinct_count\"]\n",
    "        if distinct_count > 1:\n",
    "            one_hot_encodable_columns.append(column)\n",
    "\n",
    "    # Print valid columns for one-hot encoding\n",
    "    print(\"Columns valid for one-hot encoding:\")\n",
    "    print(one_hot_encodable_columns)\n",
    "    print_execution_time(start_time, \"Checking one hot encoded columns\")\n",
    "\n",
    "    return one_hot_encodable_columns\n",
    "\n",
    "def create_preprocessing_pipeline(data, one_hot_encoded_columns, run_encoder):\n",
    "    stages = []\n",
    "\n",
    "    if run_encoder == 1:\n",
    "        # Remove columns not in DataFrame's columns\n",
    "        data_columns = data.columns\n",
    "        for value in one_hot_encoded_columns[:]:\n",
    "            if value not in data_columns:\n",
    "                print(f\"Column not in DataFrame: {value}. preprocessing pipeline\")\n",
    "                one_hot_encoded_columns.remove(value)\n",
    "\n",
    "        print(f\"Encoding {len(one_hot_encoded_columns)} columns\")\n",
    "        \n",
    "        # Add StringIndexer and OneHotEncoder stages\n",
    "        for column in one_hot_encoded_columns:\n",
    "            string_indexer = StringIndexer(inputCol=column, outputCol=column + \"_index\", handleInvalid=\"skip\")\n",
    "            encoder = OneHotEncoder(inputCols=[string_indexer.getOutputCol()], outputCols=[column + \"_vec\"])\n",
    "            stages += [string_indexer, encoder]\n",
    "\n",
    "        assembled_columns = [col + \"_vec\" for col in one_hot_encoded_columns] + \\\n",
    "                            [col for col in data.columns if col not in one_hot_encoded_columns and col != 'action_taken']\n",
    "    else:\n",
    "        # Include original columns excluding the target column\n",
    "        assembled_columns = [col for col in data.columns if col != 'action_taken']\n",
    "\n",
    "    # Add the VectorAssembler to the pipeline stages\n",
    "    assembler = VectorAssembler(inputCols=assembled_columns, outputCol=\"featuresVec\", handleInvalid=\"keep\")\n",
    "    scaler = MinMaxScaler(inputCol=\"featuresVec\", outputCol=\"features\")\n",
    "    stages.append(assembler)\n",
    "    stages.append(scaler)\n",
    "\n",
    "\n",
    "    # Create the pipeline with all stages\n",
    "    preprocessing_pipeline = Pipeline(stages=stages)\n",
    "    return preprocessing_pipeline, assembler\n",
    "\n",
    "def train_random_forest(data, assembler):\n",
    "    print(\"START RANDOM FOREST\")\n",
    "    \n",
    "    # Initialize RandomForestClassifier\n",
    "    rf_classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"action_taken\")    \n",
    "\n",
    "    # Add model to pipeline\n",
    "    pipeline = Pipeline(stages=[rf_classifier])\n",
    "\n",
    "    # Initialize variable to store the index, set to -1 as default\n",
    "    rf_classifier_index = -1\n",
    "\n",
    "    # Iterate through the pipeline stages\n",
    "    for index, stage in enumerate(pipeline.getStages()):\n",
    "        if stage.__class__.__name__ == \"RandomForestClassifier\":\n",
    "            rf_classifier_index = index\n",
    "            break\n",
    "        \n",
    "    # Split the data\n",
    "    (train_data, test_data) = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    model = pipeline.fit(train_data)\n",
    "    print_execution_time(start_time, \"Fitting model\")\n",
    "   \n",
    "    # Make predictions\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"action_taken\", predictionCol=\"prediction\")\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"precisionByLabel\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"recallByLabel\"})\n",
    "    f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "        \n",
    "    # Extract RandomForest model from the pipeline\n",
    "    rf_model = model.stages[rf_classifier_index]\n",
    "\n",
    "    # Get feature importances\n",
    "    importances = rf_model.featureImportances\n",
    "\n",
    "    # Should be feature names\n",
    "    feature_names = assembler.getInputCols()\n",
    "\n",
    "    # Zip feature names with their importances\n",
    "    feature_importance_list = [(feature, importance) for feature, importance in zip(feature_names, importances)]\n",
    "\n",
    "    # Sort the features by importance\n",
    "    sorted_features = sorted(feature_importance_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Display the sorted feature importances\n",
    "    for feature, importance in sorted_features:\n",
    "        print(f\"Feature: {feature}, Importance: {importance}\")\n",
    "         \n",
    "    print_execution_time(start_time, \"Final Time\")\n",
    "    print(\"END RANDOM FOREST\")\n",
    "    \n",
    "def train_random_forest_with_cv(data):\n",
    "    print(\"START RANDOM FOREST WITH CROSS VALIDATION\")\n",
    "    \n",
    "    # Initialize RandomForestClassifier\n",
    "    rf_classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"action_taken\")\n",
    "    \n",
    "    # Add model to pipeline\n",
    "    pipeline = Pipeline(stages=[rf_classifier])\n",
    "\n",
    "    # Define parameter grid for RandomForestClassifier\n",
    "    paramGrid = (ParamGridBuilder()\n",
    "                 .addGrid(rf_classifier.numTrees, [10, 20, 50])\n",
    "                 .addGrid(rf_classifier.maxDepth, [5, 10, 20])\n",
    "                 .build())\n",
    "\n",
    "    # Initialize an evaluator for multiclass classification\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"action_taken\", predictionCol=\"prediction\")\n",
    "\n",
    "    # Initialize CrossValidator\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator,\n",
    "                              numFolds=3)\n",
    "\n",
    "    # Split the data\n",
    "    (train_data, test_data) = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # Train the model using CrossValidator\n",
    "    start_time = time.time()\n",
    "    cvModel = crossval.fit(train_data)\n",
    "    print_execution_time(start_time, \"Fitting model\")\n",
    "    \n",
    "    # Best model from CrossValidator\n",
    "    best_model = cvModel.bestModel\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = best_model.transform(test_data)\n",
    "    \n",
    "    rf_model = best_model.stages[-1]  # Assuming RandomForestClassifier is the last stage\n",
    "    \n",
    "    # Print the parameters of the best RandomForest model\n",
    "    print(\"Best Model Parameters:\")\n",
    "    print(\"-\" * 30)\n",
    "    for param, value in rf_model.extractParamMap().items():\n",
    "        print(f\"{param.name}: {value}\")\n",
    "\n",
    "    print_execution_time(start_time, \"Final Time\")\n",
    "    print(\"END RANDOM FOREST WITH CROSS VALIDATION\")\n",
    "    \n",
    "def train_logistic_regression(data):\n",
    "    print(\"START LOGISTIC REGRESSION\")\n",
    "    \n",
    "    # Initialize LogisticRegression\n",
    "    lr_classifier = LogisticRegression(featuresCol=\"features\", \n",
    "                              labelCol=\"action_taken\", \n",
    "                              regParam=0.01, \n",
    "                              maxIter=10, \n",
    "                              elasticNetParam=0.0)\n",
    "\n",
    "    # Define the pipeline with local stages\n",
    "    pipeline = Pipeline(stages=[lr_classifier])\n",
    "\n",
    "    # Split the data\n",
    "    (train_data, test_data) = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    model = pipeline.fit(train_data)\n",
    "    print_execution_time(start_time, \"Fitting model\")\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"action_taken\", predictionCol=\"prediction\")\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"precisionByLabel\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"recallByLabel\"})\n",
    "    f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print_execution_time(start_time, \"Final time\")\n",
    "\n",
    "    print(\"END LOGISTIC REGRESSION\")\n",
    "    \n",
    "def train_logistic_regression_with_cv(data):\n",
    "    print(\"START LOGISTIC REGRESSION\")\n",
    "    \n",
    "    # Initialize LogisticRegression\n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"action_taken\")\n",
    "\n",
    "    # Define the parameter grid\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "        .addGrid(lr.maxIter, [10, 100]) \\\n",
    "        .addGrid(lr.elasticNetParam, [0.0, 0.5]) \\\n",
    "        .build()\n",
    "    \n",
    "    # Supposedly best metric for logistic regression\n",
    "    roc_evaluator = BinaryClassificationEvaluator(labelCol=\"action_taken\", metricName=\"areaUnderROC\")\n",
    "\n",
    "    # Create the CrossValidator\n",
    "    cv = CrossValidator(estimator=lr,\n",
    "                        estimatorParamMaps=paramGrid,\n",
    "                        evaluator=roc_evaluator,\n",
    "                        numFolds=3)\n",
    "\n",
    "    # Split the data\n",
    "    (train_data, test_data) = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # Train the model using CrossValidator\n",
    "    start_time = time.time()\n",
    "    cvModel = cv.fit(train_data)\n",
    "    print_execution_time(start_time, \"Fitting model\")\n",
    "\n",
    "    # Get the best model\n",
    "    bestModel = cvModel.bestModel\n",
    "\n",
    "    # Make predictions on test data\n",
    "    predictions = bestModel.transform(test_data)\n",
    "\n",
    "    # Define a new evaluator for other metrics\n",
    "    multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"action_taken\", predictionCol=\"prediction\")\n",
    "\n",
    "    # Evaluate the best model\n",
    "    accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "    precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
    "    f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
    "\n",
    "    print(f\"Best Model Parameters: regParam = {bestModel._java_obj.getRegParam()}, maxIter = {bestModel._java_obj.getMaxIter()}, elasticNetParam = {bestModel._java_obj.getElasticNetParam()}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    \n",
    "    print_execution_time(start_time, \"Final time\")\n",
    "\n",
    "    print(\"END LOGISTIC REGRESSION WITH CROSS-VALIDATION\")\n",
    "\n",
    "    return bestModel\n",
    "\n",
    "def train_xgboost_classifier(data):\n",
    "    print(\"START XGBOOST CLASSIFICATION\")\n",
    "    \n",
    "    # Initialize the XGBoost classifier\n",
    "    xgb_classifier = SparkXGBClassifier(features_col=\"features\", label_col=\"action_taken\", gamma=1.0, max_depth=5,\n",
    "    min_child_weight=5,)\n",
    "\n",
    "    # Define the pipeline with local stages\n",
    "    pipeline = Pipeline(stages=[xgb_classifier])\n",
    "\n",
    "    # Split the data\n",
    "    (train_data, test_data) = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    model = pipeline.fit(train_data)\n",
    "    print_execution_time(start_time, \"Fitting model\")\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"action_taken\", predictionCol=\"prediction\")\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "    f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print_execution_time(start_time, \"Final time\")\n",
    "\n",
    "    print(\"END XGBOOST CLASSIFICATION\")\n",
    "    \n",
    "def train_xgboost_with_cv(data):\n",
    "    print(\"START XGBOOST CLASSIFICATION WITH CROSS-VALIDATION\")\n",
    "    \n",
    "    # Initialize the XGBoost classifier\n",
    "    xgb_classifier = SparkXGBClassifier(features_col=\"features\", label_col=\"action_taken\")\n",
    "\n",
    "    # Define the pipeline with local stages\n",
    "    pipeline = Pipeline(stages=[xgb_classifier])\n",
    "\n",
    "    # Create a parameter grid for tuning the classifier\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(xgb_classifier.max_depth, [3, 5, 7]) \\\n",
    "        .addGrid(xgb_classifier.min_child_weight, [1, 5, 10]) \\\n",
    "        .addGrid(xgb_classifier.gamma, [0.1, 0.5, 1.0]) \\\n",
    "        .build()\n",
    "\n",
    "\n",
    "    # Create an evaluator for the classifier\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"action_taken\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "    # Set up cross-validation\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator,\n",
    "                              numFolds=3)\n",
    "\n",
    "    # Train the model using cross-validation\n",
    "    start_time = time.time()\n",
    "    cvModel = crossval.fit(data)\n",
    "    print_execution_time(start_time, \"Fitting model\")\n",
    "\n",
    "\n",
    "    # Fetch best model\n",
    "    bestModel = cvModel.bestModel\n",
    "    \n",
    "    try:\n",
    "        bestModel = cvModel.bestModel\n",
    "        bestXGBModel = bestModel.stages[-1]  # Assuming XGBClassifier is the last stage in the pipeline\n",
    "        \n",
    "        # Extract parameter values from the best model\n",
    "        paramMap = bestXGBModel.extractParamMap()\n",
    "        params = {param.name: value for param, value in paramMap.items()}\n",
    "        \n",
    "        print(\"Best Model Parameters:\")\n",
    "        for paramName, paramValue in params.items():\n",
    "            print(f\"{paramName}: {paramValue}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving model parameters:\", e)\n",
    "        \n",
    "    bestModelPredictions = bestModel.transform(data)\n",
    "    accuracy = evaluator.evaluate(bestModelPredictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(bestModelPredictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(bestModelPredictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "    f1 = evaluator.evaluate(bestModelPredictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "    print(f\"Best Model Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    \n",
    "    print_execution_time(start_time, \"Cross-validation training\")\n",
    "\n",
    "    print(\"END XGBOOST CLASSIFICATION WITH CROSS-VALIDATION\")\n",
    "    return bestModel\n",
    "\n",
    "def train_multilayer_perceptron_with_cv(data):\n",
    "    print(\"START MULTILAYER PERCEPTRON WITH CROSS-VALIDATION\")\n",
    "\n",
    "    # Initialize MultilayerPerceptronClassifier\n",
    "    mlp = MultilayerPerceptronClassifier(featuresCol=\"features\", labelCol=\"action_taken\")\n",
    "    \n",
    "    # num_features is first layer, 2 is last last representing 0,1 values in action_taken\n",
    "    num_features = len(data.select(\"features\").first()[0])\n",
    "    print(\"Number of features:\", num_features)\n",
    "\n",
    "    # Define the parameter grid\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(mlp.maxIter, [100, 200]) \\\n",
    "        .addGrid(mlp.layers, [[num_features, 5, 2], [num_features, 4, 3, 2]]) \\\n",
    "        .build()\n",
    "\n",
    "    # Evaluator\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"action_taken\", predictionCol=\"prediction\")\n",
    "\n",
    "    # CrossValidator\n",
    "    cv = CrossValidator(estimator=mlp,\n",
    "                        estimatorParamMaps=paramGrid,\n",
    "                        evaluator=evaluator,\n",
    "                        numFolds=3)\n",
    "\n",
    "    # Split the data\n",
    "    (train_data, test_data) = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # Train model using CrossValidator\n",
    "    start_time = time.time()\n",
    "    cvModel = cv.fit(train_data)\n",
    "    print_execution_time(start_time, \"Fitting model\")\n",
    "\n",
    "\n",
    "    # Best model\n",
    "    bestModel = cvModel.bestModel\n",
    "\n",
    "    # Make predictions and evaluate\n",
    "    predictions = bestModel.transform(test_data)\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "\n",
    "    # Print best model params and metrics\n",
    "    print(\"Best Model Parameters:\")\n",
    "    print(f\"MaxIter: {bestModel.getMaxIter()}\")\n",
    "\n",
    "    # Converting layers to a list for a readable format\n",
    "    layers = bestModel.getOrDefault(bestModel.layers)\n",
    "    print(f\"Layers: {layers}\")\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "    print_execution_time(start_time, \"Total Time\")\n",
    "    print(\"END MULTILAYER PERCEPTRON WITH CROSS-VALIDATION\")\n",
    "\n",
    "    return bestModel\n",
    "\n",
    "def train_multilayer_perceptron(data):\n",
    "    print(\"START MULTILAYER PERCEPTRON\")\n",
    "    \n",
    "    num_features = len(data.select(\"features\").first()[0])\n",
    "    print(\"Number of features:\", num_features)\n",
    "\n",
    "    # Define the classifier\n",
    "    mlp_classifier = MultilayerPerceptronClassifier(featuresCol=\"features\", \n",
    "                                                    labelCol=\"action_taken\", \n",
    "                                                    layers=[num_features, 4, 3, 2], \n",
    "                                                    maxIter=100)\n",
    "\n",
    "    # Split the data\n",
    "    (train_data, test_data) = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    model = mlp_classifier.fit(train_data)\n",
    "    print_execution_time(start_time, \"Fitting model\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"action_taken\", predictionCol=\"prediction\")\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "    f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print_execution_time(start_time, \"Final time\")\n",
    "\n",
    "    print(\"END MULTILAYER PERCEPTRON\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def columns_exceeding_null_threshold(dataframe, column_names, threshold_percentage):\n",
    "    \"\"\"\n",
    "    Identify columns in the Spark DataFrame where the percentage of null values exceeds the given threshold.\n",
    "\n",
    "    :param dataframe: The Spark DataFrame to check.\n",
    "    :param column_names: A list of column names to check for null values.\n",
    "    :param threshold_percentage: The percentage threshold for considering high null values.\n",
    "    :return: A list of column names where the null percentage exceeds the threshold.\n",
    "    \"\"\"\n",
    "    exceeding_columns = []\n",
    "    total_rows = dataframe.count()\n",
    "\n",
    "    for column_name in column_names:\n",
    "        if column_name not in dataframe.columns:\n",
    "            raise ValueError(f\"Column '{column_name}' not found in DataFrame\")\n",
    "        \n",
    "        null_count = dataframe.filter(col(column_name).isNull()).count()\n",
    "        null_percentage = (null_count / total_rows) * 100\n",
    "        if null_percentage >= threshold_percentage:\n",
    "            exceeding_columns.append(column_name)\n",
    "    \n",
    "    return exceeding_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3f08c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set local variables\n",
    "\n",
    "columns_to_drop = [\n",
    "        'activity_year','derived_msa_md','county_code','census_tract','derived_ethnicity','derived_race','derived_sex',\n",
    "        'lei','applicant_ethnicity_2','applicant_ethnicity_3','applicant_ethnicity_4','applicant_ethnicity_5',\n",
    "        'co_applicant_ethnicity_2','co_applicant_ethnicity_3','co_applicant_ethnicity_4','co_applicant_ethnicity_5',\n",
    "        'applicant_ethnicity_observed','co_applicant_ethnicity_observed','applicant_race_2','applicant_race_3',\n",
    "        'applicant_race_4','applicant_race_5','co_applicant_race_2','co_applicant_race_3','co_applicant_race_4',\n",
    "        'co_applicant_race_5','applicant_race_observed','co_applicant_race_observed','applicant_sex_observed',\n",
    "        'co_applicant_sex_observed','applicant_age_above_62','co_applicant_age_above_62','submission_of_application',\n",
    "        'initially_payable_to_institution','aus_2','aus_3','aus_4','aus_5','denial_reason_1','denial_reason_2',\n",
    "        'denial_reason_3','denial_reason_4','tract_population'\n",
    "]\n",
    "\n",
    "# Columns exhibit a high prevalence of NaN values or unexpected levels of importance in the model\n",
    "columns_to_drop_test = [\n",
    "    'combined_loan_to_value_ratio', 'discount_points', 'interest_rate', 'intro_rate_period',\n",
    "    'lender_credits', 'multifamily_affordable_units', 'open_end_line_of_credit', 'origination_charges',\n",
    "    'other_nonamortizing_features', 'preapproval', 'prepayment_penalty_term', 'rate_spread',\n",
    "    'reverse_mortgage', 'total_loan_costs', 'total_points_and_fees'\n",
    "]\n",
    "\n",
    "columns_with_all_nulls = [\n",
    "    'conforming_loan_limit',\n",
    "    'derived_loan_product_type',\n",
    "    'derived_dwelling_category',\n",
    "    'derived_ethnicity',\n",
    "    'derived_race',\n",
    "    'derived_sex',\n",
    "    'lei',\n",
    "    'state_code',\n",
    "    'co_applicant_age_above_62'\n",
    "]\n",
    "\n",
    "# Columns to remove 'Exempt'\n",
    "columns_to_replace = ['combined_loan_to_value_ratio', 'interest_rate',\n",
    "                      'debt_to_income_ratio', 'rate_spread', 'property_value']\n",
    "\n",
    "# Columns to drop NaN values\n",
    "columns_to_drop_na = ['applicant_ethnicity_1', 'income', \n",
    "                      'combined_loan_to_value_ratio', 'interest_rate', \n",
    "                      'property_value', 'loan_term', 'debt_to_income_ratio', 'action_taken']\n",
    "\n",
    "# Columns to one hot encode\n",
    "one_hot_encoded_columns = [\n",
    "    'derived_loan_product_type', 'derived_dwelling_category', 'purchaser_type', 'preapproval', \n",
    "    'loan_type', 'loan_purpose', 'lien_status', 'reverse_mortgage', 'open_end_line_of_credit', \n",
    "    'business_or_commercial_purpose', 'hoepa_status', 'negative_amortization', \n",
    "    'interest_only_payment', 'balloon_payment', 'other_nonamortizing_features', \n",
    "    'construction_method', 'occupancy_type', 'manufactured_home_secured_property_type', \n",
    "    'manufactured_home_land_property_interest', 'total_units', 'ageapplicant', \n",
    "    'debt_to_income_ratio', 'applicant_credit_score_type', 'co_applicant_credit_score_type', \n",
    "    'applicant_ethnicity_1', 'applicant_race_1', 'co_applicant_race_1', 'applicant_sex', \n",
    "    'co_applicant_sex', 'applicant_age', 'co_applicant_age', 'aus_1'\n",
    "]\n",
    "\n",
    "columns_to_impute = [\n",
    "    'business_or_commercial_purpose', 'debt_to_income_ratio', 'ffiec_msa_md_median_family_income', \n",
    "    'income', 'loan_amount', 'loan_purpose', 'loan_term', 'loan_type', 'negative_amortization', \n",
    "    'property_value', 'tract_median_age_of_housing_units', 'tract_minority_population_percent', \n",
    "    'tract_one_to_four_family_homes', 'tract_owner_occupied_units', 'tract_to_msa_income_percentage',\n",
    "    'applicant_age', 'co_applicant_age'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a9791fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|Language| Users|\n",
      "+--------+------+\n",
      "|    Java| 20000|\n",
      "|  Python|100000|\n",
      "|   Scala|  3000|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple spark test to test config.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Simple Spark Test\") \\\n",
    "    .getOrCreate()\n",
    "data = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "columns = [\"Language\", \"Users\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a6d80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Spark session\n",
    "start_session = time.time()\n",
    "spark = initialize_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c084ee57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'derived_ethnicity' not found in DataFrame. drop_columns\n",
      "Column 'derived_race' not found in DataFrame. drop_columns\n",
      "Column 'derived_sex' not found in DataFrame. drop_columns\n",
      "Column 'lei' not found in DataFrame. drop_columns\n",
      "Column 'co_applicant_age_above_62' not found in DataFrame. drop_columns\n",
      "Column 'combined_loan_to_value_ratio' not found in DataFrame. remove_exempt\n",
      "Column 'interest_rate' not found in DataFrame. remove_exempt\n",
      "Column 'rate_spread' not found in DataFrame. remove_exempt\n",
      "Column combined_loan_to_value_ratio not found in DataFrame. remove_na_rows\n",
      "Column interest_rate not found in DataFrame. remove_na_rows\n",
      "Preprocessing took: 00:01:13\n",
      "Final list of columns and rows after pre-processing\n",
      "Columns left:\n",
      "['action_taken', 'applicant_age', 'applicant_credit_score_type', 'applicant_ethnicity_1', 'applicant_race_1', 'applicant_sex', 'aus_1', 'balloon_payment', 'business_or_commercial_purpose', 'co_applicant_age', 'co_applicant_credit_score_type', 'co_applicant_ethnicity_1', 'co_applicant_race_1', 'co_applicant_sex', 'construction_method', 'debt_to_income_ratio', 'downpayment', 'downpayment_percentage', 'downpayment_to_income_ratio', 'ffiec_msa_md_median_family_income', 'hoepa_status', 'income', 'interest_only_payment', 'lien_status', 'loan_amount', 'loan_purpose', 'loan_term', 'loan_to_income_ratio', 'loan_type', 'manufactured_home_land_property_interest', 'manufactured_home_secured_property_type', 'negative_amortization', 'occupancy_type', 'property_value', 'purchaser_type', 'total_units', 'tract_median_age_of_housing_units', 'tract_minority_population_percent', 'tract_one_to_four_family_homes', 'tract_owner_occupied_units', 'tract_to_msa_income_percentage']\n",
      "Number of Columns 41\n",
      "Column not in DataFrame: derived_loan_product_type. preprocessing pipeline\n",
      "Column not in DataFrame: derived_dwelling_category. preprocessing pipeline\n",
      "Column not in DataFrame: preapproval. preprocessing pipeline\n",
      "Column not in DataFrame: reverse_mortgage. preprocessing pipeline\n",
      "Column not in DataFrame: open_end_line_of_credit. preprocessing pipeline\n",
      "Column not in DataFrame: other_nonamortizing_features. preprocessing pipeline\n",
      "Column not in DataFrame: ageapplicant. preprocessing pipeline\n",
      "Encoding 25 columns\n",
      "action_taken == 0: 2489062\n",
      "+------------+-------+\n",
      "|action_taken|  count|\n",
      "+------------+-------+\n",
      "|           0|2489062|\n",
      "|           1|2490002|\n",
      "+------------+-------+\n",
      "\n",
      "Total number of rows in the DataFrame: 4,979,064\n"
     ]
    }
   ],
   "source": [
    "# Read the data in Spark\n",
    "data = spark.read.csv(\"2021_public_lar_one_year_csv.csv\", header=True)\n",
    "\n",
    "# Preprocessing step time\n",
    "start_time = time.time()\n",
    "\n",
    "# Drop columns for various reasons\n",
    "data = drop_columns(data, columns_to_drop)\n",
    "data = drop_columns(data, columns_to_drop_test)\n",
    "data = drop_columns(data, columns_with_all_nulls)\n",
    "\n",
    "# Replace 'except'\n",
    "data = remove_exempt(data, columns_to_replace)\n",
    "\n",
    "# Drop NA rows\n",
    "data = remove_na_rows(data, columns_to_drop_na)\n",
    "\n",
    "# Find and drop all columns with all nulls values, if necessary\n",
    "#data = drop_all_null_columns(data)\n",
    "\n",
    "# Convert columns\n",
    "data = convert_columns_to_numeric(data)\n",
    "\n",
    "# Convert action_values\n",
    "data = transform_and_filter_action_taken(data)\n",
    "\n",
    "# Create new columns\n",
    "data = data.withColumn(\"downpayment\", col('property_value') - col('loan_amount'))\n",
    "data = data.withColumn('downpayment_to_income_ratio', col('downpayment') / col('income'))\n",
    "data = data.withColumn('loan_to_income_ratio', col('loan_amount') / col('income'))\n",
    "data = data.withColumn('downpayment_percentage', col('downpayment') / col('property_value'))\n",
    "data = data.filter(col(\"loan_to_income_ratio\") <= 5000000)\n",
    "             \n",
    "# Impute data if necessary\n",
    "data = impute_and_replace_columns(data, columns_to_impute)\n",
    "\n",
    "# Imputed columns won't have nulls, don't check\n",
    "#check_for_null = [col for col in data.columns if col not in columns_to_impute]\n",
    "#threshold = 40\n",
    "# Find columns that have greater than threshold% of nulls\n",
    "#exceeding_columns = columns_exceeding_null_threshold(data, check_for_null, threshold)\n",
    "#print(f\"Columns exceeding {threshold}% null values: {exceeding_columns}\")\n",
    "\n",
    "for column in data.columns:\n",
    "    data=data.dropna(subset=column)\n",
    "\n",
    "#data = data.fillna(0)\n",
    "#print_row_count(data)\n",
    "\n",
    "\n",
    "print_execution_time(start_time, \"Preprocessing\")\n",
    "\n",
    "# Use this to verify OHE columns have atleast 2 non-null values. Expensive function\n",
    "#one_hot_encoded_columns = find_columns_for_one_hot_encoding(data,one_hot_encoded_columns)\n",
    "\n",
    "print(\"Final list of columns and rows after pre-processing\")\n",
    "print_dataframe_info(data)\n",
    "\n",
    "# Assemble the data\n",
    "preprocessing_pipeline, assembler = create_preprocessing_pipeline(data, one_hot_encoded_columns, 1)\n",
    "preprocessed_data = preprocessing_pipeline.fit(data).transform(data)\n",
    " \n",
    "# Drop all columns except for label and vector\n",
    "preprocessed_data = preprocessed_data.select(\"features\", \"action_taken\")\n",
    "\n",
    "# Balance the classes\n",
    "count_class_0 = preprocessed_data.filter(preprocessed_data[\"action_taken\"] == 0).count()\n",
    "print(f\"action_taken == 0: {count_class_0}\")\n",
    "preprocessed_data = balance_classes(preprocessed_data, \"action_taken\", count_class_0)\n",
    "\n",
    "unique_value_counts = count_unique_values(preprocessed_data, \"action_taken\")\n",
    "unique_value_counts.show()\n",
    "print_row_count(preprocessed_data)\n",
    "  \n",
    "# Smaller DF if needed\n",
    "sampled_data = preprocessed_data.sample(withReplacement=False, fraction=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26238940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START RANDOM FOREST\n",
      "Fitting model took: 01:01:50\n",
      "Total number of features: 151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timj3\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Precision: 0.9653334854025195\n",
      "Overall Recall: 0.9627537950640318\n",
      "Overall F1 Score: 0.962700593154407\n",
      "Class 0.0 Precision: 0.9307395117724717\n",
      "Class 0.0 Recall: 1.0\n",
      "Class 0.0 F1 Score: 0.9641274818248552\n",
      "Class 1.0 Precision: 1.0\n",
      "Class 1.0 Recall: 0.9254294876305552\n",
      "Class 1.0 F1 Score: 0.9612707124054635\n",
      "Feature: applicant_age_vec, Importance: 0.3193672966778928\n",
      "Feature: co_applicant_sex_vec, Importance: 0.21663356468817985\n",
      "Feature: purchaser_type_vec, Importance: 0.1408917665019696\n",
      "Feature: loan_type_vec, Importance: 0.08747930178835626\n",
      "Feature: loan_purpose_vec, Importance: 0.08432806468116692\n",
      "Feature: lien_status_vec, Importance: 0.017558836331046997\n",
      "Feature: business_or_commercial_purpose_vec, Importance: 0.009146862453765147\n",
      "Feature: ffiec_msa_md_median_family_income, Importance: 0.005094156463632877\n",
      "Feature: downpayment_percentage, Importance: 0.004805949225583769\n",
      "Feature: applicant_race_1_vec, Importance: 0.0038960134669265164\n",
      "Feature: hoepa_status_vec, Importance: 0.003849784906701208\n",
      "Feature: loan_amount, Importance: 0.0037555734637448645\n",
      "Feature: negative_amortization_vec, Importance: 0.0025257120471546316\n",
      "Feature: income, Importance: 0.0020631571055163525\n",
      "Feature: tract_minority_population_percent, Importance: 0.001532961606366505\n",
      "Feature: tract_median_age_of_housing_units, Importance: 0.001244198335676405\n",
      "Feature: co_applicant_race_1_vec, Importance: 0.0007373176322789711\n",
      "Feature: occupancy_type_vec, Importance: 0.0004657224334434277\n",
      "Feature: co_applicant_credit_score_type_vec, Importance: 0.00035062399161074446\n",
      "Feature: applicant_credit_score_type_vec, Importance: 0.0002329044194321154\n",
      "Feature: co_applicant_ethnicity_1, Importance: 0.00022999453514648541\n",
      "Feature: manufactured_home_secured_property_type_vec, Importance: 0.00010906056410600404\n",
      "Feature: loan_to_income_ratio, Importance: 2.5513914786687106e-05\n",
      "Feature: applicant_ethnicity_1_vec, Importance: 5.51830753690138e-06\n",
      "Feature: interest_only_payment_vec, Importance: 4.871785029043838e-06\n",
      "Feature: total_units_vec, Importance: 4.360759467189876e-06\n",
      "Feature: downpayment_to_income_ratio, Importance: 3.177391225181373e-06\n",
      "Feature: property_value, Importance: 2.6938868272344074e-06\n",
      "Feature: balloon_payment_vec, Importance: 0.0\n",
      "Feature: construction_method_vec, Importance: 0.0\n",
      "Feature: manufactured_home_land_property_interest_vec, Importance: 0.0\n",
      "Feature: debt_to_income_ratio_vec, Importance: 0.0\n",
      "Feature: applicant_sex_vec, Importance: 0.0\n",
      "Feature: co_applicant_age_vec, Importance: 0.0\n",
      "Feature: aus_1_vec, Importance: 0.0\n",
      "Feature: downpayment, Importance: 0.0\n",
      "Feature: loan_term, Importance: 0.0\n",
      "Feature: tract_one_to_four_family_homes, Importance: 0.0\n",
      "Feature: tract_owner_occupied_units, Importance: 0.0\n",
      "Feature: tract_to_msa_income_percentage, Importance: 0.0\n",
      "Final Time took: 01:38:05\n",
      "END RANDOM FOREST\n"
     ]
    }
   ],
   "source": [
    "train_random_forest(preprocessed_data, assembler)\n",
    "#train_random_forest_with_cv(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c2c563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START LOGISTIC REGRESSION\n",
      "Fitting model took: 00:32:22\n",
      "Accuracy: 0.9678823187301333\n",
      "Precision: 0.9401553134542164\n",
      "Recall: 0.9994389564590138\n",
      "F1 Score: 0.9678494174813996\n",
      "Final time took: 01:35:25\n",
      "END LOGISTIC REGRESSION\n"
     ]
    }
   ],
   "source": [
    "train_logistic_regression(sampled_data)\n",
    "#train_logistic_regression_with_cv(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7de50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_xgboost_classifier(preprocessed_data)\n",
    "# #train_xgboost_with_cv(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bde7259a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START MULTILAYER PERCEPTRON\n",
      "Number of features: 151\n",
      "Fitting model took: 00:23:21\n",
      "Accuracy: 0.9690317128139085\n",
      "Precision: 0.9705402827811467\n",
      "Recall: 0.9690317128139085\n",
      "F1 Score: 0.9690067986869753\n",
      "Final time took: 01:35:16\n",
      "END MULTILAYER PERCEPTRON\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultilayerPerceptronClassificationModel: uid=MultilayerPerceptronClassifier_5836deaf98a0, numLayers=4, numClasses=2, numFeatures=151"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_multilayer_perceptron(sampled_data)\n",
    "#train_multilayer_perceptron_with_cv(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7799faf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Session runtime took: 02:47:24\n"
     ]
    }
   ],
   "source": [
    "# Stop the Spark session\n",
    "print_execution_time(start_session, \"Total Session runtime\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9784954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
